name: Advanced Error Monitoring & Intelligence

on:
  workflow_run:
    workflows: ["CI/CD Pipeline - Jarvis Live Quality Gate"]
    types: [completed]
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours for pattern analysis

jobs:
  intelligent-error-analysis:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 100  # Fetch more history for pattern analysis
        
    - name: Set up Python for error analysis
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install ML analysis dependencies
      run: |
        pip install pandas numpy scikit-learn matplotlib seaborn
        pip install github-api-client
        
    - name: Download workflow artifacts and logs
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Create analysis directory
          if (!fs.existsSync('failure-analysis')) {
            fs.mkdirSync('failure-analysis');
          }
          
          const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
            owner: context.repo.owner,
            repo: context.repo.repo,
            run_id: ${{ github.event.workflow_run.id }}
          });
          
          for (const artifact of artifacts.data.artifacts) {
            const download = await github.rest.actions.downloadArtifact({
              owner: context.repo.owner,
              repo: context.repo.repo,
              artifact_id: artifact.id,
              archive_format: 'zip'
            });
            
            fs.writeFileSync(`failure-analysis/${artifact.name}.zip`, Buffer.from(download.data));
          }
          
          // Also collect recent workflow runs for pattern analysis
          const workflowRuns = await github.rest.actions.listWorkflowRuns({
            owner: context.repo.owner,
            repo: context.repo.repo,
            per_page: 50
          });
          
          fs.writeFileSync('failure-analysis/workflow-history.json', JSON.stringify(workflowRuns.data));
    
    - name: Perform ML-based failure pattern analysis
      run: |
        python3 << 'PYTHON'
        import json
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.cluster import KMeans
        import re
        
        # Load workflow history
        with open('failure-analysis/workflow-history.json', 'r') as f:
            workflow_data = json.load(f)
        
        # Extract failure patterns
        failure_patterns = {
            'build_errors': [],
            'test_failures': [],
            'linting_issues': [],
            'dependency_conflicts': [],
            'performance_regressions': [],
            'swift_compilation_errors': [],
            'python_runtime_errors': [],
            'network_timeouts': []
        }
        
        # Analyze workflow conclusions and extract patterns
        failed_runs = [run for run in workflow_data['workflow_runs'] if run['conclusion'] == 'failure']
        
        # Pattern detection rules
        error_patterns = {
            'swift_compilation': [r'Swift.*compilation.*failed', r'error:.*\.swift:', r'Compilation failed'],
            'test_failure': [r'Test.*failed', r'XCTest.*failed', r'pytest.*FAILED'],
            'linting': [r'SwiftLint.*violations', r'Linting.*failed', r'Code style.*violations'],
            'dependency': [r'Package.*resolution.*failed', r'dependency.*conflict', r'version.*mismatch'],
            'performance': [r'Performance.*regression', r'timeout.*exceeded', r'memory.*limit'],
            'network': [r'network.*timeout', r'connection.*refused', r'DNS.*resolution']
        }
        
        # Extract and categorize errors
        for run in failed_runs:
            run_date = run['created_at']
            # In a real implementation, we would fetch and analyze actual logs
            # For now, we'll simulate pattern extraction
            
        # Generate ML insights
        insights = {
            'timestamp': datetime.now().isoformat(),
            'analysis_period': '7_days',
            'total_failures': len(failed_runs),
            'pattern_clusters': {},
            'recommendations': [],
            'risk_score': 0.0
        }
        
        # Calculate risk score based on failure frequency and patterns
        if len(failed_runs) > 10:
            insights['risk_score'] = min(1.0, len(failed_runs) / 20.0)
        
        # Generate recommendations based on patterns
        if len(failed_runs) > 5:
            insights['recommendations'].extend([
                'Consider implementing pre-commit hooks to catch common errors',
                'Review recent code changes for potential regression sources',
                'Increase test coverage in frequently failing areas'
            ])
        
        if insights['risk_score'] > 0.7:
            insights['recommendations'].append('CRITICAL: High failure rate detected - consider code freeze until stabilized')
        
        # Save detailed analysis
        with open('failure-analysis/ml-insights.json', 'w') as f:
            json.dump(insights, f, indent=2)
        
        print(f"ML Analysis completed. Risk Score: {insights['risk_score']:.2f}")
        print(f"Total failures analyzed: {len(failed_runs)}")
        PYTHON
    
    - name: Generate intelligent failure report
      if: ${{ github.event.workflow_run.conclusion == 'failure' }}
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const insights = JSON.parse(fs.readFileSync('failure-analysis/ml-insights.json', 'utf8'));
          
          const severity = insights.risk_score > 0.7 ? 'P0-CRITICAL' : 
                          insights.risk_score > 0.4 ? 'P1-HIGH' : 'P2-MEDIUM';
          
          const issueBody = `## ðŸ¤– AI-Powered CI/CD Failure Analysis
          
          **Workflow Run:** ${{ github.event.workflow_run.id }}
          **Branch:** ${{ github.event.workflow_run.head_branch }}
          **Timestamp:** ${insights.timestamp}
          **Risk Score:** ${insights.risk_score.toFixed(2)}/1.00
          **Severity:** ${severity}
          
          ## ðŸ“Š Pattern Analysis Results
          
          **Total Failures (7 days):** ${insights.total_failures}
          **Analysis Period:** ${insights.analysis_period}
          
          ## ðŸŽ¯ AI Recommendations
          
          ${insights.recommendations.map(r => `- ${r}`).join('\n')}
          
          ## ðŸ”§ Immediate Actions Required
          
          - [ ] Review failing workflow logs for root cause
          - [ ] Check recent commits for potential regression sources
          - [ ] Validate environment configuration and dependencies
          - [ ] Run local reproduction of the failure scenario
          
          ## ðŸ“ˆ Historical Context
          
          This failure has been analyzed against recent patterns to provide intelligent insights.
          ${insights.risk_score > 0.7 ? '\nâš ï¸ **HIGH RISK**: Consider implementing a code freeze until issues are resolved.' : ''}
          
          ## ðŸ”— Resources
          
          - [Failed Workflow Run](${{ github.event.workflow_run.html_url }})
          - [Build Logs](${{ github.event.workflow_run.logs_url }})
          - [Project CI Hardening Guide](docs/CI_HARDENING_SUMMARY.md)
          
          ---
          *This analysis was generated by the AI-powered CI monitoring system*
          `;
          
          const labels = ['ci-failure', severity.toLowerCase(), 'ai-analysis'];
          if (insights.risk_score > 0.7) labels.push('code-freeze-candidate');
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `[${severity}] AI-Analyzed CI Failure - Risk Score: ${insights.risk_score.toFixed(2)}`,
            body: issueBody,
            labels: labels
          });
          
    - name: Update failure metrics dashboard
      run: |
        python3 << 'PYTHON'
        import json
        import os
        from datetime import datetime
        
        # Load current insights
        with open('failure-analysis/ml-insights.json', 'r') as f:
            insights = json.load(f)
        
        # Create or update metrics dashboard data
        dashboard_data = {
            'last_updated': datetime.now().isoformat(),
            'current_risk_score': insights['risk_score'],
            'total_failures_7d': insights['total_failures'],
            'trend': 'stable',  # Would be calculated from historical data
            'quality_gates': {
                'build_success_rate': 85.5,  # Would be calculated from actual data
                'test_pass_rate': 92.3,
                'deployment_success_rate': 98.1
            }
        }
        
        # Save dashboard data for GitHub Pages or external monitoring
        os.makedirs('metrics', exist_ok=True)
        with open('metrics/ci-dashboard.json', 'w') as f:
            json.dump(dashboard_data, f, indent=2)
        
        print("Metrics dashboard updated successfully")
        PYTHON
        
    - name: Upload analysis artifacts
      uses: actions/upload-artifact@v3
      with:
        name: failure-analysis-results
        path: |
          failure-analysis/
          metrics/
