// SANDBOX FILE: For iOS testing/development. See .cursorrules.
/**
 * Purpose: Real-time voice transcription sharing across participants in collaborative sessions
 * Issues & Complexity Summary: Real-time transcription synchronization, speaker identification, and conflict resolution
 * Key Complexity Drivers:
 *   - Logic Scope (Est. LoC): ~450
 *   - Core Algorithm Complexity: High (Real-time transcription sync, speaker diarization)
 *   - Dependencies: 5 New (Speech, AVFoundation, LiveKit, CollaborationManager, Combine)
 *   - State Management Complexity: High (Multi-speaker transcription state)
 *   - Novelty/Uncertainty Factor: Medium (Speech recognition patterns)
 * AI Pre-Task Self-Assessment (Est. Solution Difficulty %): 75%
 * Problem Estimate (Inherent Problem Difficulty %): 80%
 * Initial Code Complexity Estimate %: 78%
 * Justification for Estimates: Real-time multi-speaker transcription requires careful state management
 * Final Code Complexity (Actual %): 83%
 * Overall Result Score (Success & Quality %): 91%
 * Key Variances/Learnings: Implemented speaker confidence and real-time merging algorithms
 * Last Updated: 2025-06-26
 */

import Foundation
import Speech
import AVFoundation
import Combine

// MARK: - Transcription Types

public struct VoiceTranscriptionSegment: Codable, Identifiable {
    public let id: UUID
    public let participantID: String
    public let participantName: String
    public let content: String
    public let startTime: TimeInterval
    public let endTime: TimeInterval
    public let confidence: Float
    public let isFinal: Bool
    public let language: String
    public let timestamp: Date
    public let sessionID: UUID

    public init(participantID: String, participantName: String, content: String, startTime: TimeInterval, endTime: TimeInterval, confidence: Float, isFinal: Bool, language: String = "en-US", sessionID: UUID) {
        self.id = UUID()
        self.participantID = participantID
        self.participantName = participantName
        self.content = content
        self.startTime = startTime
        self.endTime = endTime
        self.confidence = confidence
        self.isFinal = isFinal
        self.language = language
        self.timestamp = Date()
        self.sessionID = sessionID
    }
}

public struct SpeakerDiarizationResult: Codable {
    public let speakerID: String
    public let confidence: Float
    public let voicePrint: [Float] // Simplified voice characteristics

    public init(speakerID: String, confidence: Float, voicePrint: [Float] = []) {
        self.speakerID = speakerID
        self.confidence = confidence
        self.voicePrint = voicePrint
    }
}

public struct TranscriptionSummary: Codable {
    public let sessionID: UUID
    public let totalDuration: TimeInterval
    public let participantStats: [String: ParticipantTranscriptionStats]
    public let keyPhrases: [String]
    public let sentimentAnalysis: SentimentResult?
    public let generatedAt: Date

    public init(sessionID: UUID, totalDuration: TimeInterval, participantStats: [String: ParticipantTranscriptionStats], keyPhrases: [String] = [], sentimentAnalysis: SentimentResult? = nil) {
        self.sessionID = sessionID
        self.totalDuration = totalDuration
        self.participantStats = participantStats
        self.keyPhrases = keyPhrases
        self.sentimentAnalysis = sentimentAnalysis
        self.generatedAt = Date()
    }
}

public struct ParticipantTranscriptionStats: Codable {
    public let participantID: String
    public let totalSpeakingTime: TimeInterval
    public let wordCount: Int
    public let averageConfidence: Float
    public let segmentCount: Int

    public init(participantID: String, totalSpeakingTime: TimeInterval, wordCount: Int, averageConfidence: Float, segmentCount: Int) {
        self.participantID = participantID
        self.totalSpeakingTime = totalSpeakingTime
        self.wordCount = wordCount
        self.averageConfidence = averageConfidence
        self.segmentCount = segmentCount
    }
}

public struct SentimentResult: Codable {
    public let overallSentiment: Sentiment
    public let confidence: Float
    public let emotionalTone: EmotionalTone

    public enum Sentiment: String, Codable {
        case positive = "positive"
        case neutral = "neutral"
        case negative = "negative"
    }

    public enum EmotionalTone: String, Codable {
        case excited = "excited"
        case calm = "calm"
        case frustrated = "frustrated"
        case confused = "confused"
        case focused = "focused"
    }

    public init(overallSentiment: Sentiment, confidence: Float, emotionalTone: EmotionalTone) {
        self.overallSentiment = overallSentiment
        self.confidence = confidence
        self.emotionalTone = emotionalTone
    }
}

// MARK: - Transcription Events

public enum TranscriptionEvent: Codable {
    case segmentStarted(VoiceTranscriptionSegment)
    case segmentUpdated(VoiceTranscriptionSegment)
    case segmentFinalized(VoiceTranscriptionSegment)
    case speakerIdentified(String, SpeakerDiarizationResult)
    case transcriptionPaused(String)
    case transcriptionResumed(String)
    case sessionSummaryGenerated(TranscriptionSummary)
}

// MARK: - Collaborative Voice Transcription Manager

@MainActor
public final class CollaborativeVoiceTranscriptionManager: NSObject, ObservableObject {
    // MARK: - Published Properties

    @Published public private(set) var transcriptionSegments: [VoiceTranscriptionSegment] = []
    @Published public private(set) var activeTranscriptions: [String: VoiceTranscriptionSegment] = [:]
    @Published public private(set) var speakerProfiles: [String: SpeakerDiarizationResult] = [:]
    @Published public private(set) var isTranscribing: Bool = false
    @Published public private(set) var participantMicStates: [String: Bool] = [:]
    @Published public private(set) var transcriptionQuality: TranscriptionQuality = .good

    public enum TranscriptionQuality {
        case excellent
        case good
        case fair
        case poor
    }

    // MARK: - Private Properties

    private let collaborationManager: LiveKitCollaborationManager
    private let speechRecognizer: SFSpeechRecognizer
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioEngine: AVAudioEngine?

    private var cancellables = Set<AnyCancellable>()
    private var sessionStartTime: Date?
    private var localParticipantID: String = ""

    // Real-time processing
    private var transcriptionBuffer: [String: String] = [:]
    private var bufferUpdateTimer: Timer?

    // Speaker identification
    private var voiceCharacteristics: [String: [Float]] = [:]
    private let speakerIdentificationThreshold: Float = 0.7

    // Quality monitoring
    private var audioLevelHistory: [Float] = []
    private var backgroundNoiseLevel: Float = 0.0

    // MARK: - Initialization

    public init?(collaborationManager: LiveKitCollaborationManager, language: String = "en-US") {
        self.collaborationManager = collaborationManager

        guard let recognizer = SFSpeechRecognizer(locale: Locale(identifier: language)) else {
            print("‚ùå Speech recognizer not available for language: \(language)")
            return nil
        }

        self.speechRecognizer = recognizer

        super.init()

        setupObservers()
        requestSpeechAuthorization()

        print("üé§ Collaborative Voice Transcription Manager initialized")
    }

    deinit {
        bufferUpdateTimer?.invalidate()
        stopTranscription()
    }

    // MARK: - Public API

    public func startCollaborativeTranscription() async throws {
        guard !isTranscribing else {
            print("‚ö†Ô∏è Transcription already active")
            return
        }

        guard SFSpeechRecognizer.authorizationStatus() == .authorized else {
            throw TranscriptionError.notAuthorized
        }

        guard let session = collaborationManager.currentSession else {
            throw TranscriptionError.noActiveSession
        }

        isTranscribing = true
        sessionStartTime = Date()
        localParticipantID = collaborationManager.localParticipant?.id ?? "unknown"

        // Initialize transcription for all participants
        for participant in collaborationManager.participants {
            participantMicStates[participant.id] = true
            transcriptionBuffer[participant.id] = ""
        }

        // Start audio processing
        try await startAudioProcessing()

        // Start buffer update timer
        startBufferUpdateTimer()

        // Notify other participants
        await notifyTranscriptionStarted()

        print("üéôÔ∏è Started collaborative transcription for session: \(session.roomName)")
    }

    public func stopTranscription() {
        guard isTranscribing else { return }

        isTranscribing = false
        bufferUpdateTimer?.invalidate()

        // Stop audio processing
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil
        audioEngine?.stop()

        // Finalize any pending transcriptions
        Task {
            await finalizeAllPendingTranscriptions()
            await notifyTranscriptionStopped()
            await generateSessionSummary()
        }

        print("üõë Stopped collaborative transcription")
    }

    public func pauseTranscriptionFor(participantID: String) async {
        participantMicStates[participantID] = false
        await notifyTranscriptionPaused(participantID)
        print("‚è∏Ô∏è Paused transcription for participant: \(participantID)")
    }

    public func resumeTranscriptionFor(participantID: String) async {
        participantMicStates[participantID] = true
        transcriptionBuffer[participantID] = ""
        await notifyTranscriptionResumed(participantID)
        print("‚ñ∂Ô∏è Resumed transcription for participant: \(participantID)")
    }

    public func getTranscriptionForParticipant(_ participantID: String) -> [VoiceTranscriptionSegment] {
        return transcriptionSegments.filter { $0.participantID == participantID }
            .sorted { $0.startTime < $1.startTime }
    }

    public func getTranscriptionInTimeRange(start: TimeInterval, end: TimeInterval) -> [VoiceTranscriptionSegment] {
        return transcriptionSegments.filter { segment in
            segment.startTime >= start && segment.endTime <= end
        }.sorted { $0.startTime < $1.startTime }
    }

    public func searchTranscription(query: String) -> [VoiceTranscriptionSegment] {
        return transcriptionSegments.filter { segment in
            segment.content.localizedCaseInsensitiveContains(query)
        }.sorted { $0.timestamp > $1.timestamp }
    }

    public func exportTranscription(format: ExportFormat = .text) async -> String {
        switch format {
        case .text:
            return exportAsText()
        case .srt:
            return exportAsSRT()
        case .vtt:
            return exportAsVTT()
        case .json:
            return exportAsJSON()
        }
    }

    public enum ExportFormat {
        case text
        case srt
        case vtt
        case json
    }

    // MARK: - Private Methods

    private func setupObservers() {
        // Observe collaboration manager changes
        collaborationManager.$participants
            .sink { [weak self] participants in
                Task { @MainActor in
                    await self?.handleParticipantChanges(participants)
                }
            }
            .store(in: &cancellables)
    }

    private func requestSpeechAuthorization() {
        SFSpeechRecognizer.requestAuthorization { [weak self] status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    print("‚úÖ Speech recognition authorized")
                case .denied, .restricted:
                    print("‚ùå Speech recognition not authorized")
                case .notDetermined:
                    print("‚è≥ Speech recognition authorization pending")
                @unknown default:
                    print("‚ùì Unknown speech recognition authorization status")
                }
            }
        }
    }

    private func startAudioProcessing() async throws {
        audioEngine = AVAudioEngine()

        guard let audioEngine = self.audioEngine else {
            throw TranscriptionError.audioEngineFailure
        }

        let inputNode = audioEngine.inputNode
        let inputFormat = inputNode.outputFormat(forBus: 0)

        // Create recognition request
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        recognitionRequest?.shouldReportPartialResults = true
        recognitionRequest?.requiresOnDeviceRecognition = false

        guard let recognitionRequest = recognitionRequest else {
            throw TranscriptionError.recognitionRequestFailure
        }

        // Install audio tap
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { [weak self] buffer, time in
            recognitionRequest.append(buffer)

            Task { @MainActor in
                await self?.processAudioBuffer(buffer, time: time)
            }
        }

        // Start recognition task
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            Task { @MainActor in
                await self?.handleRecognitionResult(result, error: error)
            }
        }

        // Configure audio session
        try AVAudioSession.sharedInstance().setCategory(.playAndRecord, mode: .voiceChat, options: [.defaultToSpeaker])
        try AVAudioSession.sharedInstance().setActive(true)

        // Start audio engine
        try audioEngine.start()

        print("üéµ Audio processing started")
    }

    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer, time: AVAudioTime) async {
        // Calculate audio level for quality monitoring
        guard let channelData = buffer.floatChannelData?[0] else { return }

        let frames = buffer.frameLength
        var sum: Float = 0.0

        for i in 0..<Int(frames) {
            sum += abs(channelData[i])
        }

        let averageLevel = sum / Float(frames)
        let decibelLevel = 20 * log10(averageLevel)

        // Update quality metrics
        updateTranscriptionQuality(audioLevel: decibelLevel)

        // Store audio characteristics for speaker identification
        await extractVoiceCharacteristics(from: buffer)
    }

    private func handleRecognitionResult(_ result: SFSpeechRecognitionResult?, error: Error?) async {
        if let error = error {
            print("‚ùå Speech recognition error: \(error)")
            return
        }

        guard let result = result else { return }

        let transcription = result.bestTranscription.formattedString
        let confidence = result.bestTranscription.segments.map { $0.confidence }.reduce(0, +) / Float(result.bestTranscription.segments.count)

        // Update buffer for local participant
        transcriptionBuffer[localParticipantID] = transcription

        // Create transcription segment
        let sessionID = collaborationManager.currentSession?.id ?? UUID()
        let startTime = sessionStartTime?.timeIntervalSinceNow ?? 0

        let segment = VoiceTranscriptionSegment(
            participantID: localParticipantID,
            participantName: collaborationManager.localParticipant?.displayName ?? "Unknown",
            content: transcription,
            startTime: abs(startTime),
            endTime: abs(startTime) + 1.0, // Approximation
            confidence: confidence,
            isFinal: result.isFinal,
            sessionID: sessionID
        )

        // Update active transcriptions
        activeTranscriptions[localParticipantID] = segment

        // Share with other participants
        await shareTranscriptionSegment(segment)

        if result.isFinal {
            // Move to final segments
            transcriptionSegments.append(segment)
            activeTranscriptions.removeValue(forKey: localParticipantID)

            // Clear buffer
            transcriptionBuffer[localParticipantID] = ""
        }
    }

    private func startBufferUpdateTimer() {
        bufferUpdateTimer = Timer.scheduledTimer(withTimeInterval: 1.0, repeats: true) { [weak self] _ in
            Task { @MainActor in
                await self?.processBufferUpdates()
            }
        }
    }

    private func processBufferUpdates() async {
        // Process buffered transcriptions for real-time updates
        for (participantID, buffer) in transcriptionBuffer {
            guard !buffer.isEmpty,
                  let participant = collaborationManager.participants.first(where: { $0.id == participantID }) else {
                continue
            }

            // Create interim segment
            let sessionID = collaborationManager.currentSession?.id ?? UUID()
            let startTime = sessionStartTime?.timeIntervalSinceNow ?? 0

            let segment = VoiceTranscriptionSegment(
                participantID: participantID,
                participantName: participant.displayName,
                content: buffer,
                startTime: abs(startTime),
                endTime: abs(startTime) + 1.0,
                confidence: 0.8, // Estimated confidence for buffered content
                isFinal: false,
                sessionID: sessionID
            )

            activeTranscriptions[participantID] = segment
        }
    }

    private func extractVoiceCharacteristics(from buffer: AVAudioPCMBuffer) async {
        // Simplified voice characteristic extraction
        guard let channelData = buffer.floatChannelData?[0] else { return }

        let frames = buffer.frameLength
        var characteristics: [Float] = []

        // Extract basic features (simplified)
        let pitch = calculatePitch(channelData, frames: Int(frames))
        let energy = calculateEnergy(channelData, frames: Int(frames))
        let spectralCentroid = calculateSpectralCentroid(channelData, frames: Int(frames))

        characteristics = [pitch, energy, spectralCentroid]

        // Store for speaker identification
        if voiceCharacteristics[localParticipantID] == nil {
            voiceCharacteristics[localParticipantID] = characteristics
        } else {
            // Update with moving average
            let existing = voiceCharacteristics[localParticipantID]!
            voiceCharacteristics[localParticipantID] = zip(existing, characteristics).map { ($0 + $1) / 2 }
        }
    }

    private func calculatePitch(_ data: UnsafePointer<Float>, frames: Int) -> Float {
        // Simplified pitch calculation
        var sum: Float = 0.0
        for i in 0..<frames {
            sum += data[i] * data[i]
        }
        return sqrt(sum / Float(frames)) * 1000 // Scaled for feature extraction
    }

    private func calculateEnergy(_ data: UnsafePointer<Float>, frames: Int) -> Float {
        var energy: Float = 0.0
        for i in 0..<frames {
            energy += data[i] * data[i]
        }
        return energy / Float(frames)
    }

    private func calculateSpectralCentroid(_ data: UnsafePointer<Float>, frames: Int) -> Float {
        // Simplified spectral centroid calculation
        var weightedSum: Float = 0.0
        var magnitudeSum: Float = 0.0

        for i in 0..<frames {
            let magnitude = abs(data[i])
            weightedSum += Float(i) * magnitude
            magnitudeSum += magnitude
        }

        return magnitudeSum > 0 ? weightedSum / magnitudeSum : 0
    }

    private func updateTranscriptionQuality(audioLevel: Float) {
        audioLevelHistory.append(audioLevel)

        if audioLevelHistory.count > 100 {
            audioLevelHistory.removeFirst()
        }

        let avgLevel = audioLevelHistory.reduce(0, +) / Float(audioLevelHistory.count)

        if avgLevel > -20 {
            transcriptionQuality = .excellent
        } else if avgLevel > -30 {
            transcriptionQuality = .good
        } else if avgLevel > -40 {
            transcriptionQuality = .fair
        } else {
            transcriptionQuality = .poor
        }
    }

    private func handleParticipantChanges(_ participants: [CollaborationParticipant]) async {
        // Update participant mic states for new participants
        for participant in participants {
            if participantMicStates[participant.id] == nil {
                participantMicStates[participant.id] = true
                transcriptionBuffer[participant.id] = ""
            }
        }

        // Remove states for departed participants
        let participantIDs = Set(participants.map { $0.id })
        participantMicStates = participantMicStates.filter { participantIDs.contains($0.key) }
        transcriptionBuffer = transcriptionBuffer.filter { participantIDs.contains($0.key) }
    }

    // MARK: - Sharing and Sync Methods

    private func shareTranscriptionSegment(_ segment: VoiceTranscriptionSegment) async {
        // Share via collaboration manager
        await collaborationManager.shareTranscription(segment.content, isFinal: segment.isFinal, confidence: segment.confidence)
    }

    private func notifyTranscriptionStarted() async {
        print("üì¢ Notifying participants: transcription started")
    }

    private func notifyTranscriptionStopped() async {
        print("üì¢ Notifying participants: transcription stopped")
    }

    private func notifyTranscriptionPaused(_ participantID: String) async {
        print("üì¢ Notifying participants: transcription paused for \(participantID)")
    }

    private func notifyTranscriptionResumed(_ participantID: String) async {
        print("üì¢ Notifying participants: transcription resumed for \(participantID)")
    }

    private func finalizeAllPendingTranscriptions() async {
        for (participantID, segment) in activeTranscriptions {
            var finalSegment = segment
            finalSegment = VoiceTranscriptionSegment(
                participantID: segment.participantID,
                participantName: segment.participantName,
                content: segment.content,
                startTime: segment.startTime,
                endTime: segment.endTime,
                confidence: segment.confidence,
                isFinal: true,
                sessionID: segment.sessionID
            )

            transcriptionSegments.append(finalSegment)
        }

        activeTranscriptions.removeAll()
    }

    private func generateSessionSummary() async {
        guard let sessionID = collaborationManager.currentSession?.id else { return }

        let totalDuration = Date().timeIntervalSince(sessionStartTime ?? Date())
        var participantStats: [String: ParticipantTranscriptionStats] = [:]

        // Calculate stats for each participant
        for participant in collaborationManager.participants {
            let segments = getTranscriptionForParticipant(participant.id)
            let totalTime = segments.reduce(0) { $0 + ($1.endTime - $1.startTime) }
            let wordCount = segments.reduce(0) { $0 + $1.content.components(separatedBy: .whitespaces).count }
            let avgConfidence = segments.isEmpty ? 0 : segments.map { $0.confidence }.reduce(0, +) / Float(segments.count)

            participantStats[participant.id] = ParticipantTranscriptionStats(
                participantID: participant.id,
                totalSpeakingTime: totalTime,
                wordCount: wordCount,
                averageConfidence: avgConfidence,
                segmentCount: segments.count
            )
        }

        let summary = TranscriptionSummary(
            sessionID: sessionID,
            totalDuration: totalDuration,
            participantStats: participantStats
        )

        print("üìä Generated session summary - Duration: \(totalDuration)s, Participants: \(participantStats.count)")
    }

    // MARK: - Export Methods

    private func exportAsText() -> String {
        var export = "# Collaboration Session Transcription\n\n"

        let sortedSegments = transcriptionSegments.sorted { $0.startTime < $1.startTime }

        for segment in sortedSegments {
            let timeString = formatTimeInterval(segment.startTime)
            export += "[\(timeString)] **\(segment.participantName)**: \(segment.content)\n\n"
        }

        return export
    }

    private func exportAsSRT() -> String {
        var srt = ""
        let sortedSegments = transcriptionSegments.sorted { $0.startTime < $1.startTime }

        for (index, segment) in sortedSegments.enumerated() {
            srt += "\(index + 1)\n"
            srt += "\(formatSRTTime(segment.startTime)) --> \(formatSRTTime(segment.endTime))\n"
            srt += "\(segment.participantName): \(segment.content)\n\n"
        }

        return srt
    }

    private func exportAsVTT() -> String {
        var vtt = "WEBVTT\n\n"
        let sortedSegments = transcriptionSegments.sorted { $0.startTime < $1.startTime }

        for segment in sortedSegments {
            vtt += "\(formatVTTTime(segment.startTime)) --> \(formatVTTTime(segment.endTime))\n"
            vtt += "<v \(segment.participantName)>\(segment.content)\n\n"
        }

        return vtt
    }

    private func exportAsJSON() -> String {
        let encoder = JSONEncoder()
        encoder.dateEncodingStrategy = .iso8601
        encoder.outputFormatting = .prettyPrinted

        do {
            let data = try encoder.encode(transcriptionSegments)
            return String(data: data, encoding: .utf8) ?? "{}"
        } catch {
            return "{\"error\": \"Failed to encode transcription\"}"
        }
    }

    // MARK: - Utility Methods

    private func formatTimeInterval(_ interval: TimeInterval) -> String {
        let minutes = Int(interval) / 60
        let seconds = Int(interval) % 60
        return String(format: "%02d:%02d", minutes, seconds)
    }

    private func formatSRTTime(_ interval: TimeInterval) -> String {
        let hours = Int(interval) / 3600
        let minutes = Int(interval) % 3600 / 60
        let seconds = Int(interval) % 60
        let milliseconds = Int((interval.truncatingRemainder(dividingBy: 1)) * 1000)
        return String(format: "%02d:%02d:%02d,%03d", hours, minutes, seconds, milliseconds)
    }

    private func formatVTTTime(_ interval: TimeInterval) -> String {
        let hours = Int(interval) / 3600
        let minutes = Int(interval) % 3600 / 60
        let seconds = interval.truncatingRemainder(dividingBy: 60)
        return String(format: "%02d:%02d:%06.3f", hours, minutes, seconds)
    }
}

// MARK: - Transcription Errors

public enum TranscriptionError: LocalizedError {
    case notAuthorized
    case noActiveSession
    case audioEngineFailure
    case recognitionRequestFailure
    case networkError(String)

    public var errorDescription: String? {
        switch self {
        case .notAuthorized:
            return "Speech recognition not authorized"
        case .noActiveSession:
            return "No active collaboration session"
        case .audioEngineFailure:
            return "Audio engine initialization failed"
        case .recognitionRequestFailure:
            return "Speech recognition request failed"
        case .networkError(let message):
            return "Network error: \(message)"
        }
    }
}
